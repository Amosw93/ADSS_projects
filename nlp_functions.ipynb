{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99d0b76-c77f-48b5-a68d-6752dd001ad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global Functions for ADSS sentiment analysis project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2bd00-e5f8-479e-a5c2-b1483ba96128",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. extract_pdf_text(pdf_file)\n",
    "Function: Extracting text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2840bd-24dd-4393-a37b-6b8a1bf351c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_file):\n",
    "    \n",
    "    \"\"\"Extracts text from a PDF\n",
    "    \n",
    "    Args:\n",
    "        pdf_file: the path to the pdf file.\n",
    "    \n",
    "    Steps:\n",
    "    Part A. Extract sentence from pdf\n",
    "        1. Read the pdf document\n",
    "        2. Create a pdf reader\n",
    "        3. Get the total numbers of pages of the pdf document\n",
    "        4. Extract text from each pages of the pdf document\n",
    "        5. Split the text into sentence by full stop\n",
    "\n",
    "    Part B. Clean the sentence\n",
    "        1. Remove empty sentence\n",
    "        2. Remove newline indicator /n\n",
    "        3. Replace mutiple conseuctive spaces with single space\n",
    "        4. Replace double quotation marks \"\" with signle quotation marks\n",
    "\n",
    "    Return:\n",
    "        A list of numbers of cleanned sentence from the pdf documents\n",
    "    \"\"\"\n",
    "    # libraries\n",
    "    import re # regular expression library to handle sentence pattern\n",
    "    import PyPDF2 # pdf library \n",
    "    \n",
    "    # Create a list to store the extracted text from each page\n",
    "    sentences = []\n",
    "    \n",
    "    ### Part A. Extract text from pdf\n",
    "    with open(pdf_file, \"rb\") as f:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        # Iteration over each page of the PDF and extract the text content\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "        \n",
    "            # split the text into sentences\n",
    "            sentences.extend(text.split('.'))\n",
    "    \n",
    "    ### Part B. Clean the sentence\n",
    "    # Remove any empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    # Remove any /n indicating newline characters\n",
    "    sentences = [sentence.replace('\\n', ' ') for sentence in sentences]\n",
    "    \n",
    "    # Replace any mulitple consecutive spaces with a single space '\\s+'\n",
    "    sentences = [re.sub(r'\\s+', ' ', sentence) for sentence in sentences]\n",
    "    \n",
    "    # Replace any double quotation marks \"\" with a single\n",
    "    sentences = [re.sub(r'\"\"', '\"', sentence) for sentence in sentences]\n",
    "\n",
    "    return sentences\n",
    "    # End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d17064-9b95-4196-8711-ee59a13c0e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. standardize_sentences(extracted_sentences)\n",
    "Function: lowering, removing punctuation, numbers, stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2643db7-33ab-4cbe-a4a0-1dd9f8946ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_sentences(extracted_sentences):\n",
    "    \"\"\"\n",
    "    Lowercases all texts, removes punctuation, numbers, and stopwords.\n",
    "\n",
    "    Args:\n",
    "        extracted_sentences: Extracted and cleaned sentences from the PDF.\n",
    "\n",
    "    Steps:\n",
    "        1. Convert text to lowercase\n",
    "        2. Remove punctuation and numbers\n",
    "        3. Remove stopwords such as \"the,\" \"and,\" \"is,\" \"a,\" which are commonly used\n",
    "           without any sentiment.\n",
    "\n",
    "    Return:\n",
    "        A list of sentences with all lowercase text,\n",
    "        without punctuation, numbers, and stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    import re  # regular expression library to handle sentence pattern\n",
    "    from nltk.corpus import stopwords  # natural language toolkit stopwords module\n",
    "\n",
    "    clean_sentences = []  # storing the cleaned sentences\n",
    "    stop_words = set(stopwords.words('english'))  # using English stopwords\n",
    "\n",
    "    # iteration over each extracted sentence and then preprocess the sentence\n",
    "    for sentence in extracted_sentences:\n",
    "        # Convert text to lowercase\n",
    "        lower_text = sentence.lower()\n",
    "\n",
    "        # Remove punctuation and numbers\n",
    "        # [^a-zA-Z] regular expression for non-English characters\n",
    "        removed_punc_num = re.sub('[^a-zA-Z]', ' ', lower_text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        # iteration over each word and check whether it matches with the stopwords\n",
    "        # If the word does not match with the stopword, then join into a sentence using a single separator \" \"\n",
    "        clean_sentence = ' '.join(word for word in removed_punc_num.split() if word not in stop_words)\n",
    "\n",
    "        # Append the preprocessed sentence to the list\n",
    "        clean_sentences.append(clean_sentence)\n",
    "\n",
    "    return clean_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6394087-1814-4627-9c85-1039f5e8ce7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. label_sentences(sentences, word_list)\n",
    "Function: labeling the sentences as negative, positive, or neutral based on the given word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a366fa2-2130-40d9-a42d-46bb301b2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(sentences, word_list):\n",
    "    \"\"\"\n",
    "    Labels sentences as negative, positive, or neutral based on the given word list.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences.\n",
    "        word_list: DataFrame containing financial words and their sentiment.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with sentences and labels.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    labels = []\n",
    "    positive_words = set(word_list[word_list['sentiment'] == 'positive']['word'])\n",
    "    negative_words = set(word_list[word_list['sentiment'] == 'negative']['word'])\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if any(word in positive_words for word in words):\n",
    "            labels.append('positive')\n",
    "        elif any(word in negative_words for word in words):\n",
    "            labels.append('negative')\n",
    "        else:\n",
    "            labels.append('neutral')\n",
    "\n",
    "    # Create a DataFrame with sentences and labels\n",
    "    data = pd.DataFrame({'sentences': sentences, 'labels': labels})\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4c4b7-3be8-4d98-b84d-bb1f1de4fdbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. convert_labels_to_numeric(data, label_mapping)\n",
    "\n",
    "Function: Converting categorical labels into numeric values based on the provided label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b93971-5e4e-43c6-a2d8-f47bc4ad623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_numeric(data, label_mapping):\n",
    "    \"\"\"\n",
    "    Converts categorical labels into numeric values based on the provided label mapping.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing sentences and labels.\n",
    "        label_mapping (dict): Mapping of categorical labels to numeric values.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with labels converted to numeric values.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original data\n",
    "    converted_data = data.copy()\n",
    "    \n",
    "    # Convert labels to numeric values\n",
    "    converted_data['labels'] = converted_data['labels'].map(label_mapping)\n",
    "    \n",
    "    return converted_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba015cc-088b-491b-8df8-3f1f08990f98",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. split_the_dataset(dataset, training_size)\n",
    "Function: Spliting the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2757383-e095-4ce9-b3bc-c81112c8c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_the_dataset(dataset, training_size):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to be split.\n",
    "        training_size: The proportion of the dataset to be used for training.\n",
    "\n",
    "    Returns:\n",
    "        Two datasets: the training dataset and the testing dataset.\n",
    "    \"\"\"\n",
    "    # library\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    training_data, testing_data = train_test_split(dataset, \n",
    "                                                   train_size=training_size, \n",
    "                                                   random_state=42)\n",
    "    \n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f46da4-ba01-44d3-9cb8-6bd39b5e5298",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. tokenize_pad_sequences(sentences, num_words, oov_token, maxlen, padding, truncating)\n",
    "Function: Tokenize the text from sentences and pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54512f04-8140-4a34-8561-e19eeef4a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pad_sequences(sentences, num_words, oov_token, maxlen, padding, truncating):\n",
    "    \"\"\"Tokenize the text from sentences and pad the sequences\n",
    "    Args:\n",
    "        1. sentences: A list of cleaned and preprocessed sentences.\n",
    "        2. num_words: An integer specifying the maximum number of words \n",
    "        to keep based on word frequency.\n",
    "        3. oov_token: A string specifying the out-of-vocabulary token to \n",
    "        be used for words not present in the tokenizer's word index.\n",
    "        4. maxlen: An integer specifying the maximum length of the sequences.\n",
    "        5. padding: A string specifying the padding type to use. It can be either 'pre' or 'post'.\n",
    "        6. truncating: A string specifying the truncation type to use. It can be \n",
    "        either 'pre' or 'post'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # library\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    # Initialize the Tokenizer class\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    \n",
    "    # Split each sentence into words\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    # Generate indices for each word in the corpus\n",
    "    tokenizer.fit_on_texts(tokenized_sentences)\n",
    "\n",
    "    # Get the indices\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # Generate list of token sequences\n",
    "    sequences = tokenizer.texts_to_sequences(tokenized_sentences)\n",
    "    \n",
    "    # Pad the sequences with the assigned length, padding, and truncating\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n",
    "    \n",
    "    return tokenizer, word_index, sequences, padded_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031bb96-735f-4908-b67e-8f4b5adcd8cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. summary_tokpad(padded_sentences, start_index, end_index)\n",
    "Function: Printing a summary of the tokenization and padding process for a given range of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f5267b-595b-4221-b209-79ad4e8b0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_tokpad(padded_sentences, start_index, end_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints a summary of the tokenization and padding process for a given range of entries.\n",
    "\n",
    "    Args:\n",
    "        padded_sentences: A tuple containing the word index, sequences, and padded sequences.\n",
    "        start_index: The starting index of the entries to be summarized.\n",
    "        end_index: The ending index of the entries to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        None (Prints the summary to the console).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the word index, sequences, and padded sequences\n",
    "    word_index = padded_sentences[1]\n",
    "    sequences = padded_sentences[2]\n",
    "    padded_sequences = padded_sentences[3]\n",
    "\n",
    "    # Print the assigned number of entries of the word index\n",
    "    print(f\"start index: {start_index}\")\n",
    "    print(f\"end index: {end_index}\")\n",
    "    print(\"Summary of the tokenization and padding:\")\n",
    "    print(\"\\nSelected Word Index:\")\n",
    "    for word, index in list(word_index.items())[start_index:end_index]:\n",
    "        print(f\"{word}: {index}\")\n",
    "\n",
    "    # Print the selected sentences\n",
    "    print(\"\\nSelected Sentences:\")\n",
    "    for sentence in sequences[start_index:end_index]:\n",
    "        print(sentence)\n",
    "\n",
    "    # Print the selected padded sequences\n",
    "    print(\"\\nSelected Padded Sequences:\")\n",
    "    for padded_seq in padded_sequences[start_index:end_index]:\n",
    "        print(padded_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ce99c-5b1a-4090-8aae-2b02b0447f80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. scrap_yahoo_news(stock_sym, start_date, end_date)\n",
    "Function: Scrapes Yahoo Finance news search results for a given query and date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a1075b3-9736-4c51-8527-7310bef172cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_yahoo_news(stock_sym, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes Yahoo Finance news search results for a given query and date range.\n",
    "\n",
    "    Args:\n",
    "        stock_sym (str): The stock symbol\n",
    "        start_date (str): \"YYYY-MM-DD\"\n",
    "        end_date (str): \"YYYY-MM-DD\"\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sentences extracted from the news articles.\n",
    "\n",
    "    \"\"\"\n",
    "    # Library\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "\n",
    "    # Format the query and date range in the URL\n",
    "    formatted_query = stock_sym.replace(\" \", \"+\")\n",
    "    url = f\"https://search.yahoo.com/search?p={formatted_query}+news&b={start_date}&bt={end_date}\"\n",
    "\n",
    "    # Send a GET request to the search results page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the news articles on the search results page\n",
    "    articles = soup.find_all(\"div\", class_=\"algo-sr\")\n",
    "\n",
    "    # Create a list to store the extracted content\n",
    "    contents = []\n",
    "\n",
    "    # Loop through the articles and extract the content\n",
    "    for article in articles:\n",
    "        content = article.find(\"p\").text.strip()\n",
    "        \n",
    "        # Remove the date using regex\n",
    "        content_without_date = re.sub(r\"[A-Za-z]+\\s+\\d{1,2},\\s+\\d{4}\\s*Â·\\s*\", \"\", content)\n",
    "        \n",
    "        contents.append(content_without_date)\n",
    "\n",
    "    # Combine the content into a single string\n",
    "    combined_content = ' '.join(contents)\n",
    "\n",
    "    # Split the combined content into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', combined_content)\n",
    "\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de3a29-4ea0-4fa6-83c8-483b6f1d30e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Removing pdf encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447c6ac3-f1c9-4cbe-8347-2abc705f52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pdf_encryption(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Function to remove security settings from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): The path to the input PDF file.\n",
    "        output_path (str): The path to save the output PDF file without security settings.\n",
    "    \"\"\"\n",
    "    import PyPDF2\n",
    "    \n",
    "    with open(input_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        if pdf_reader.is_encrypted:\n",
    "            pdf_reader.decrypt('')\n",
    "        \n",
    "        pdf_writer = PyPDF2.PdfWriter()\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_writer.add_page(page)\n",
    "        \n",
    "        with open(output_path, 'wb') as output_file:\n",
    "            pdf_writer.write(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
